predictors <- ceosal2[, c("lsales", "profits")]
target <- ceosal2$salary
# Handle missing data if necessary
predictors <- na.omit(predictors)
target <- target[!is.na(target)]
# Standardize predictors
scaled_predictors <- scale(predictors)
# Define weights for predictors
weights <- c(0.7, 0.3)  # Assign higher weight to 'sales' predictor, for example
# Function to calculate weighted Euclidean distance
weighted_distance <- function(x1, x2, weights) {
sqrt(sum(weights * (x1 - x2)^2))
}
# Custom KNN implementation
custom_knn <- function(train_data, test_point, train_labels, k, weights) {
distances <- apply(train_data, 1, function(row) {
weighted_distance(as.numeric(row), as.numeric(test_point), weights)
})
# Combine distances with labels
neighbors <- data.frame(distance = distances, label = train_labels)
# Get the K nearest neighbors
nearest_neighbors <- neighbors %>%
arrange(distance) %>%
slice(1:k)
# Return the most common label among nearest neighbors
most_common_label <- names(sort(table(nearest_neighbors$label), decreasing = TRUE))[1]
return(most_common_label)
}
# Split data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(scaled_predictors), size = 0.7 * nrow(scaled_predictors))
train_data <- scaled_predictors[train_indices, ]
test_data <- scaled_predictors[-train_indices, ]
train_labels <- target[train_indices]
test_labels <- target[-train_indices]
# Apply the custom KNN function
k <- 5
predictions <- sapply(1:nrow(test_data), function(i) {
custom_knn(train_data, test_data[i, ], train_labels, k, weights)
})
# Evaluate predictions
print(table(Predicted = predictions, Actual = test_labels))
train_control <- trainControl(method = "LOOCV")
knn_model <- train(lsalary ~ lsales + comten + ceoten, data = trainData, method = "knn", tuneLength = 10, trControl = train_control)
knn_pred <- predict(knn_model, newdata = testData)
knn_rmse <- sqrt(mean((knn_pred - testData$lsalary)^2))
knn_rmse
train_control <- trainControl(method = "LOOCV")
knn_model <- train(lsalary ~ lsales + ceoten, data = trainData, method = "knn", tuneLength = 10, trControl = train_control)
knn_pred <- predict(knn_model, newdata = testData)
knn_rmse <- sqrt(mean((knn_pred - testData$lsalary)^2))
knn_rmse
train_control <- trainControl(method = "LOOCV")
knn_model <- train(lsalary ~ lsales * 0.6+ ceoten* 0.2 * comten * 0.2, data = trainData, method = "knn", tuneLength = 10, trControl = train_control)
train_control <- trainControl(method = "LOOCV")
knn_model <- train(lsalary ~ lsales * 0.6 + ceoten* 0.2 * comten * 0.2, data = trainData, method = "knn", tuneLength = 10, trControl = train_control)
train_control <- trainControl(method = "LOOCV")
knn_model <- train(lsalary ~ (lsales * 0.6) + (ceoten* 0.2) + (comten * 0.2), data = trainData, method = "knn", tuneLength = 10, trControl = train_control)
train_control <- trainControl(method = "LOOCV")
knn_model <- train(lsalary ~ (lsales * 0.6) + (ceoten * 0.2) + (comten * 0.2), data = trainData, method = "knn", tuneLength = 10, trControl = train_control)
library(Wooldridge)  # Load the dataset
library(wooldridge)  # Load the dataset
library(caret)       # For preprocessing and model training
library(class)       # For KNN
library(FNN)         # For weighted KNN
library(fnn)         # For weighted KNN
install.packages("FNN")
library(FNN)         # For weighted KNN
# Load dataset
data("ceosal2")
# Inspect data
str(ceosal2)
# Select relevant predictors and target
ceo_data <- ceosal2[, c("lsalary", "sales", "profits", "ceoten", "mktval")]
# Handle missing values (if any)
ceo_data <- na.omit(ceo_data)
# Normalize predictors (important for KNN)
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceo_data_norm <- as.data.frame(lapply(ceo_data[, -1], normalize))
ceo_data_norm$lsalary <- ceo_data$lsalary
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceo_data_norm), 0.7 * nrow(ceo_data_norm))
train_data <- ceo_data_norm[train_indices, ]
test_data <- ceo_data_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -4], train_data$lsalary, test_data[, -4], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -4], test = test_data[, -4], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
# Normalize predictors
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceosal2_norm <- as.data.frame(lapply(ceosal2[, c("sales", "profits", "ceoten", "mktval")], normalize))
ceosal2_norm$lsalary <- ceosal2$lsalary  # Keeping target variable unchanged
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceosal2_norm), 0.7 * nrow(ceosal2_norm))
train_data <- ceosal2_norm[train_indices, ]
test_data <- ceosal2_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -1], train_data$lsalary, test_data[, -1], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
# Normalize predictors
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceosal2_norm <- as.data.frame(lapply(ceosal2[, c("sales", "profits", "ceoten", "mktval")], normalize))
ceosal2_norm$lsalary <- ceosal2$lsalary  # Keeping target variable unchanged
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceosal2_norm), 0.8 * nrow(ceosal2_norm))
train_data <- ceosal2_norm[train_indices, ]
test_data <- ceosal2_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -1], train_data$lsalary, test_data[, -1], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
# Normalize predictors
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceosal2_norm <- as.data.frame(lapply(ceosal2[, c("sales", "profits", "ceoten", "mktval")], normalize))
ceosal2_norm$lsalary <- ceosal2$lsalary  # Keeping target variable unchanged
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceosal2_norm), 0.8 * nrow(ceosal2_norm))
train_data <- ceosal2_norm[train_indices, ]
test_data <- ceosal2_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -1], train_data$lsalary, test_data[, -1], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data_norm[, -1], test = test_data_norm[, -1], y = train_data_norm$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data_norm$lsalary)^2))
})
# Normalize predictors
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceosal2_norm <- as.data.frame(lapply(ceosal2[, c("sales", "profits", "ceoten", "mktval")], normalize))
ceosal2_norm$lsalary <- ceosal2$lsalary  # Keeping target variable unchanged
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceosal2_norm), 0.8 * nrow(ceosal2_norm))
train_data <- ceosal2_norm[train_indices, ]
test_data <- ceosal2_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -1], train_data$lsalary, test_data[, -1], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data[, -1], test = test_data_norm[, -1], y = train_data_norm$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data_norm$lsalary)^2))
})
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data[, -1], test = test_data_norm[, -1], y = train_data$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data_norm$lsalary)^2))
})
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data$lsalary)^2))
})
# Plot RMSE vs K
ggplot(data.frame(k_values, rmse_values), aes(x = k_values, y = rmse_values)) +
geom_line() + geom_point() +
labs(title = "K vs RMSE", x = "Number of Neighbors (K)", y = "RMSE") +
theme_minimal()
# Importing libraries
library(tidyverse)
library(readxl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
# Read each sheet into a list of data frames, skipping the first row
list_of_dfs <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet, skip = 2)
})
# Combine all data frames into a single data frame
combined_data <- bind_rows(list_of_dfs)
# View the combined data frame
print(combined_data)
# Unique values in the first column
unique_values <- unique(combined_data$...1)
print(unique_values)
# Define the values to filter out
values_to_exclude <- c(
"Svakvin", "R??dvin", "Hvitvin", "Musserende vin", "Ros??vin",
"Perlende vin", "Aromatisert vin", "Sider", "Fruktvin",
"Brennevin", "Vodka", "Lik??r", "Whisky", "Akevitt",
"Brennevin, annet", "Gin", "Druebrennevin",
"Brennevin, n??ytralt < 37,5 %", "Rom", "Bitter",
"Fruktbrennevin", "Genever", "??l", "Alkoholfritt", "Sterkvin", "Totalsum",
"eLager"
)
# Column names of combined data
colnames(combined_data)
# Filter out the specified values from the first column
filtered_data <- combined_data %>%
mutate("2024" = as.numeric(`2024`),
"Store" = as.character(`...1`)) %>%
filter(!.[[1]] %in% values_to_exclude) %>%
select("Store", "2024")
# View the filtered data
print(filtered_data)
setwd("C:/Users/oyaod/OneDrive/Skrivebord/Spring_2025/Ban440/Eksamen/BAN440-Term-Paper/Code/Data wrangling")
# Importing libraries
library(tidyverse)
library(readxl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
# Read each sheet into a list of data frames, skipping the first row
list_of_dfs <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet, skip = 2)
})
# Combine all data frames into a single data frame
combined_data <- bind_rows(list_of_dfs)
# View the combined data frame
print(combined_data)
# Unique values in the first column
unique_values <- unique(combined_data$...1)
print(unique_values)
# Define the values to filter out
values_to_exclude <- c(
"Svakvin", "R??dvin", "Hvitvin", "Musserende vin", "Ros??vin",
"Perlende vin", "Aromatisert vin", "Sider", "Fruktvin",
"Brennevin", "Vodka", "Lik??r", "Whisky", "Akevitt",
"Brennevin, annet", "Gin", "Druebrennevin",
"Brennevin, n??ytralt < 37,5 %", "Rom", "Bitter",
"Fruktbrennevin", "Genever", "??l", "Alkoholfritt", "Sterkvin", "Totalsum",
"eLager"
)
# Column names of combined data
colnames(combined_data)
# Filter out the specified values from the first column
filtered_data <- combined_data %>%
mutate("2024" = as.numeric(`2024`),
"Store" = as.character(`...1`)) %>%
filter(!.[[1]] %in% values_to_exclude) %>%
select("Store", "2024")
# Importing libraries
library(tidyverse)
library(readxl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
setwd("C:/Users/oyaod/OneDrive/Skrivebord/Spring_2025/Ban440/Eksamen/BAN440-Term-Paper/Code/Data wrangling")
# Importing libraries
library(tidyverse)
library(readxl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
setwd("C:/Users/oyaod/OneDrive/Skrivebord/Spring_2025/Ban440/Eksamen/BAN440-Term-Paper/Data/Vinmonopolet")
# Importing libraries
library(tidyverse)
library(readxl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
# Read each sheet into a list of data frames, skipping the first row
list_of_dfs <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet, skip = 2)
})
# Combine all data frames into a single data frame
combined_data <- bind_rows(list_of_dfs)
# View the combined data frame
print(combined_data)
# Unique values in the first column
unique_values <- unique(combined_data$...1)
print(unique_values)
# Transforming to normal characters
combined_data$...1 <- iconv(combined_data$...1, from = "UTF-8", to = "ASCII//TRANSLIT")
combined_data$...1 <- trimws(combined_data$...1)
# Define the values to filter out
values_to_exclude <- c(
"Svakvin", "Rodvin", "Hvitvin", "Musserende vin", "Rosevin",
"Perlende vin", "Aromatisert vin", "Sider", "Fruktvin",
"Brennevin", "Vodka", "Likor", "Whisky", "Akevitt",
"Brennevin, annet", "Gin", "Druebrennevin",
"Brennevin, noytralt < 37,5 %", "Rom", "Bitter",
"Fruktbrennevin", "Genever", "Ol", "Alkoholfritt", "Sterkvin", "Totalsum",
"eLager"
)
# Column names of combined data
colnames(combined_data)
# Filter out the specified values from the first column
filtered_data <- combined_data %>%
mutate("2024" = as.numeric(`2024`),
"Store" = as.character(`...1`)) %>%
filter(!.[[1]] %in% values_to_exclude) %>%
select("Store", "2024")
# View the filtered data
print(filtered_data)
library(readxl)
library(dplyr)
# Load the postal code dataset
postal_data <- read_excel("dimpostnummer.xlsx")
# Rename columns to match their purpose in final_data
postal_data <- postal_data %>%
rename(
Postal_Code = `Postnummer`,           # Postal code
Municipality_Code = `KommuneKode`,    # Municipality code
Municipality_Name = `Kommune`,        # Municipality name
Region_Code = `FylkeKode`,            # Region code
Region_Name = `Fylke`,                # Region name
Latitude = `Latitude`,                # Latitude
Longitude = `Longitude`               # Longitude
)
# Convert Postal_Code to numeric to ensure a proper join
postal_data <- postal_data %>%
mutate(Postal_Code = as.numeric(Postal_Code))
setwd("C:/Users/oyaod/OneDrive/Skrivebord/Spring_2025/Ban440/Eksamen/BAN440-Term-Paper/Data/Kommunedata")
library(readxl)
library(dplyr)
# Load the postal code dataset
postal_data <- read_excel("dimpostnummer.xlsx")
# Rename columns to match their purpose in final_data
postal_data <- postal_data %>%
rename(
Postal_Code = `Postnummer`,           # Postal code
Municipality_Code = `KommuneKode`,    # Municipality code
Municipality_Name = `Kommune`,        # Municipality name
Region_Code = `FylkeKode`,            # Region code
Region_Name = `Fylke`,                # Region name
Latitude = `Latitude`,                # Latitude
Longitude = `Longitude`               # Longitude
)
# Convert Postal_Code to numeric to ensure a proper join
postal_data <- postal_data %>%
mutate(Postal_Code = as.numeric(Postal_Code))
final_data <- final_data %>%
mutate(Postal_Code = as.numeric(Postal_Code))
# Convert Postal_Code to numeric to ensure a proper join
postal_data <- postal_data %>%
mutate(Postal_Code = as.numeric(Postal_Code))
final_data <- final_data %>%
mutate(Postal_Code = as.numeric(Postal_Code))
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)
library(readr)
# Define API URL
url <- "https://apis.vinmonopolet.no/stores/v0/details"
# Define your subscription key (replace with your actual key)
subscription_key <- "3b5b02c6793240fe9e6cb6d176e110e0"
# Send GET request with subscription key in header
response <- GET(url,
add_headers(
Accept = "application/json",
`Ocp-Apim-Subscription-Key` = subscription_key  # API authentication
))
# Check response status
if (status_code(response) == 200) {
# Convert API response to JSON and store it
data <- content(response, as = "text", encoding = "UTF-8")
store_data <- fromJSON(data)
# View first few rows
print(head(store_data))
} else {
print(paste("Error:", status_code(response)))
}
# Load necessary libraries
library(dplyr)
library(stringr)
library(tidyr) # Load tidyr for unnesting
library(tidyverse)
# Ensure store_data_clean is correctly formatted
store_data_clean <- store_data %>%
unnest_wider(address) %>%  # Expands nested address fields
select(
storeId,
storeName,
status,
postalCode,
city,
gpsCoord
) %>%
rename(
Store_ID = storeId,
Store_Name = storeName,
Store_Status = status,
Postal_Code = postalCode,
City = city,
GPS_Coordinates = gpsCoord
)
# Standardize store names to improve matching
filtered_data <- filtered_data %>%
mutate(Store = str_trim(str_to_lower(Store)))  # Trim spaces and convert to lowercase
store_data_clean <- store_data_clean %>%
mutate(Store_Name = str_trim(str_to_lower(Store_Name)))  # Trim spaces and convert to lowercase
# Merge filtered_data (sales) with store_data_clean (store details)
final_data <- filtered_data %>%
left_join(store_data_clean, by = c("Store" = "Store_Name"))  # Match by store name
# Check merged data
head(final_data)
# Merge final_data (stores & revenue) with postal_data (municipality info)
final_data <- final_data %>%
left_join(postal_data, by = "Postal_Code")
### Vinmonopolet data wrangling ###
# Importing libraries
library(tidyverse)
library(readxl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
