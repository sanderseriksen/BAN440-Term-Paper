final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
# Normalize predictors
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceosal2_norm <- as.data.frame(lapply(ceosal2[, c("sales", "profits", "ceoten", "mktval")], normalize))
ceosal2_norm$lsalary <- ceosal2$lsalary  # Keeping target variable unchanged
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceosal2_norm), 0.8 * nrow(ceosal2_norm))
train_data <- ceosal2_norm[train_indices, ]
test_data <- ceosal2_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -1], train_data$lsalary, test_data[, -1], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
# Normalize predictors
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceosal2_norm <- as.data.frame(lapply(ceosal2[, c("sales", "profits", "ceoten", "mktval")], normalize))
ceosal2_norm$lsalary <- ceosal2$lsalary  # Keeping target variable unchanged
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceosal2_norm), 0.8 * nrow(ceosal2_norm))
train_data <- ceosal2_norm[train_indices, ]
test_data <- ceosal2_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -1], train_data$lsalary, test_data[, -1], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data_norm[, -1], test = test_data_norm[, -1], y = train_data_norm$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data_norm$lsalary)^2))
})
# Normalize predictors
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
ceosal2_norm <- as.data.frame(lapply(ceosal2[, c("sales", "profits", "ceoten", "mktval")], normalize))
ceosal2_norm$lsalary <- ceosal2$lsalary  # Keeping target variable unchanged
# Split into train and test sets
set.seed(123)
train_indices <- sample(1:nrow(ceosal2_norm), 0.8 * nrow(ceosal2_norm))
train_data <- ceosal2_norm[train_indices, ]
test_data <- ceosal2_norm[-train_indices, ]
# Function to find the best K value
find_best_k <- function(train_x, train_y, test_x, test_y, max_k = 20) {
rmse_values <- numeric(max_k)
for (k in 1:max_k) {
knn_model <- knn.reg(train = train_x, test = test_x, y = train_y, k = k)
rmse_values[k] <- sqrt(mean((knn_model$pred - test_y)^2))
}
best_k <- which.min(rmse_values)
return(list(best_k = best_k, min_rmse = rmse_values[best_k], rmse_values = rmse_values))
}
# Apply function to normalized data
knn_results <- find_best_k(train_data[, -1], train_data$lsalary, test_data[, -1], test_data$lsalary, max_k = 20)
# Print best K and RMSE
print(paste("Best K:", knn_results$best_k))
print(paste("Minimum RMSE:", knn_results$min_rmse))
# Train final KNN model with best K
final_knn <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = knn_results$best_k)
# Evaluate performance
final_rmse <- sqrt(mean((final_knn$pred - test_data$lsalary)^2))
print(paste("Final Model RMSE:", final_rmse))
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data[, -1], test = test_data_norm[, -1], y = train_data_norm$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data_norm$lsalary)^2))
})
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data[, -1], test = test_data_norm[, -1], y = train_data$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data_norm$lsalary)^2))
})
library(ggplot2)
# Generate RMSE plot for different K values
k_values <- 1:20
rmse_values <- sapply(k_values, function(k) {
knn_model <- knn.reg(train = train_data[, -1], test = test_data[, -1], y = train_data$lsalary, k = k)
sqrt(mean((knn_model$pred - test_data$lsalary)^2))
})
# Plot RMSE vs K
ggplot(data.frame(k_values, rmse_values), aes(x = k_values, y = rmse_values)) +
geom_line() + geom_point() +
labs(title = "K vs RMSE", x = "Number of Neighbors (K)", y = "RMSE") +
theme_minimal()
setwd("C:/Users/oyaod/OneDrive/Skrivebord/Spring_2025/Ban440/Eksamen/BAN440-Term-Paper/Data/Vinmonopolet")
### Vinmonopolet data wrangling ###
# Importing libraries
library(tidyverse)
library(readxl)
library(writexl)
### Vinmonopolet data wrangling ###
# Importing libraries
library(tidyverse)
library(readxl)
library(writexl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
# Read each sheet into a list of data frames, skipping the first row
list_of_dfs <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet, skip = 2)
})
# Combine all data frames into a single data frame
combined_data <- bind_rows(list_of_dfs)
# View the combined data frame
print(combined_data)
# Unique values in the first column
unique_values <- unique(combined_data$...1)
print(unique_values)
# Transforming to normal characters
combined_data$...1 <- iconv(combined_data$...1, from = "UTF-8", to = "ASCII//TRANSLIT")
combined_data$...1 <- trimws(combined_data$...1)
# Define the values to filter out
values_to_exclude <- c(
"Svakvin", "Rodvin", "Hvitvin", "Musserende vin", "Rosevin",
"Perlende vin", "Aromatisert vin", "Sider", "Fruktvin",
"Brennevin", "Vodka", "Likor", "Whisky", "Akevitt",
"Brennevin, annet", "Gin", "Druebrennevin",
"Brennevin, noytralt < 37,5 %", "Rom", "Bitter",
"Fruktbrennevin", "Genever", "Ol", "Alkoholfritt", "Sterkvin", "Totalsum",
"eLager"
)
# Column names of combined data
colnames(combined_data)
# Filter out the specified values from the first column
filtered_data <- combined_data %>%
mutate("2024" = as.numeric(`2024`),
"Store" = as.character(`...1`)) %>%
filter(!.[[1]] %in% values_to_exclude) %>%
select("Store", "2024")
# View the filtered data
print(filtered_data)
# Export the filtered data to an Excel file
write_xlsx(filtered_data, "filtered_data.xlsx")
# --------------------------- API INTEGRATION ---------------------------
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)
library(readr)
# Define API URL
url <- "https://apis.vinmonopolet.no/stores/v0/details"
# Define your subscription key (replace with your actual key)
subscription_key <- "3b5b02c6793240fe9e6cb6d176e110e0"
# Send GET request with subscription key in header
response <- GET(url,
add_headers(
Accept = "application/json",
`Ocp-Apim-Subscription-Key` = subscription_key  # API authentication
))
# Check response status
if (status_code(response) == 200) {
# Convert API response to JSON and store it
data <- content(response, as = "text", encoding = "UTF-8")
store_data <- fromJSON(data)
# View first few rows
print(head(store_data))
} else {
print(paste("Error:", status_code(response)))
}
# --------------------------- Combine API with Vinmonopol Data ---------------------------
# Load necessary libraries
library(dplyr)
library(stringr)
library(tidyr) # Load tidyr for unnesting
library(tidyverse)
# Ensure store_data_clean is correctly formatted
store_data_clean <- store_data %>%
unnest_wider(address) %>%  # Expands nested address fields
select(
storeId,
storeName,
status,
postalCode,
city,
gpsCoord
) %>%
rename(
Store_ID = storeId,
Store_Name = storeName,
Store_Status = status,
Postal_Code = postalCode,
City = city,
GPS_Coordinates = gpsCoord
)
# Standardize store names to improve matching
filtered_data <- filtered_data %>%
mutate(Store = str_trim(str_to_lower(Store)))  # Trim spaces and convert to lowercase
store_data_clean <- store_data_clean %>%
mutate(Store_Name = str_trim(str_to_lower(Store_Name)))  # Trim spaces and convert to lowercase
# Merge filtered_data (sales) with store_data_clean (store details)
final_data <- filtered_data %>%
left_join(store_data_clean, by = c("Store" = "Store_Name"))  # Match by store name
# Check merged data
head(final_data)
# --------------------------- API INTEGRATION ---------------------------
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)
library(readr)
# Define API URL
url <- "https://apis.vinmonopolet.no/stores/v0/details"
# Define your subscription key (replace with your actual key)
subscription_key <- "3b5b02c6793240fe9e6cb6d176e110e0"
# Send GET request with subscription key in header
response <- GET(url,
add_headers(
Accept = "application/json",
`Ocp-Apim-Subscription-Key` = subscription_key  # API authentication
))
# Check response status
if (status_code(response) == 200) {
# Convert API response to JSON and store it
data <- content(response, as = "text", encoding = "UTF-8")
store_data <- fromJSON(data)
# View first few rows
print(head(store_data))
} else {
print(paste("Error:", status_code(response)))
}
# --------------------------- Combine API with Vinmonopol Data ---------------------------
# Load necessary libraries
library(dplyr)
library(stringr)
library(tidyr) # Load tidyr for unnesting
library(tidyverse)
# Ensure store_data_clean is correctly formatted
store_data_clean <- store_data %>%
unnest_wider(address) %>%  # Expands nested address fields
select(
storeId,
storeName,
status,
postalCode,
city,
gpsCoord
) %>%
rename(
Store_ID = storeId,
Store_Name = storeName,
Store_Status = status,
Postal_Code = postalCode,
City = city,
GPS_Coordinates = gpsCoord
)
# Standardize store names to improve matching
filtered_data <- filtered_data %>%
mutate(Store = str_trim(str_to_lower(Store)))  # Trim spaces and convert to lowercase
### Vinmonopolet data wrangling ###
# Importing libraries
library(tidyverse)
library(readxl)
library(writexl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
# Read each sheet into a list of data frames, skipping the first row
list_of_dfs <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet, skip = 2)
})
# Combine all data frames into a single data frame
combined_data <- bind_rows(list_of_dfs)
# View the combined data frame
print(combined_data)
# Unique values in the first column
unique_values <- unique(combined_data$...1)
print(unique_values)
# Transforming to normal characters
combined_data$...1 <- iconv(combined_data$...1, from = "UTF-8", to = "ASCII//TRANSLIT")
combined_data$...1 <- trimws(combined_data$...1)
# Define the values to filter out
values_to_exclude <- c(
"Svakvin", "Rodvin", "Hvitvin", "Musserende vin", "Rosevin",
"Perlende vin", "Aromatisert vin", "Sider", "Fruktvin",
"Brennevin", "Vodka", "Likor", "Whisky", "Akevitt",
"Brennevin, annet", "Gin", "Druebrennevin",
"Brennevin, noytralt < 37,5 %", "Rom", "Bitter",
"Fruktbrennevin", "Genever", "Ol", "Alkoholfritt", "Sterkvin", "Totalsum",
"eLager"
)
# Column names of combined data
colnames(combined_data)
# Filter out the specified values from the first column
filtered_data <- combined_data %>%
mutate("2024" = as.numeric(`2024`),
"Store" = as.character(`...1`)) %>%
filter(!.[[1]] %in% values_to_exclude) %>%
select("Store", "2024")
# View the filtered data
print(filtered_data)
# Export the filtered data to an Excel file
write_xlsx(filtered_data, "filtered_data.xlsx")
# --------------------------- API INTEGRATION ---------------------------
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)
library(readr)
# Define API URL
url <- "https://apis.vinmonopolet.no/stores/v0/details"
# Define your subscription key (replace with your actual key)
subscription_key <- "3b5b02c6793240fe9e6cb6d176e110e0"
# Send GET request with subscription key in header
response <- GET(url,
add_headers(
Accept = "application/json",
`Ocp-Apim-Subscription-Key` = subscription_key  # API authentication
))
# Check response status
if (status_code(response) == 200) {
# Convert API response to JSON and store it
data <- content(response, as = "text", encoding = "UTF-8")
store_data <- fromJSON(data)
# View first few rows
print(head(store_data))
} else {
print(paste("Error:", status_code(response)))
}
# --------------------------- Combine API with Vinmonopol Data ---------------------------
# Load necessary libraries
library(dplyr)
library(stringr)
library(tidyr) # Load tidyr for unnesting
library(tidyverse)
# Ensure store_data_clean is correctly formatted
store_data_clean <- store_data %>%
unnest_wider(address) %>%  # Expands nested address fields
select(
storeId,
storeName,
status,
postalCode,
city,
gpsCoord
) %>%
rename(
Store_ID = storeId,
Store_Name = storeName,
Store_Status = status,
Postal_Code = postalCode,
City = city,
GPS_Coordinates = gpsCoord
)
# Standardize store names to improve matching
filtered_data <- filtered_data %>%
mutate(Store = str_trim(str_to_lower(Store)))  # Trim spaces and convert to lowercase
store_data_clean <- store_data_clean %>%
mutate(Store_Name = str_trim(str_to_lower(Store_Name)))  # Trim spaces and convert to lowercase
# Merge filtered_data (sales) with store_data_clean (store details)
final_data <- filtered_data %>%
left_join(store_data_clean, by = c("Store" = "Store_Name"))  # Match by store name
# Check merged data
head(final_data)
View(final_data)
# --------------------------- API INTEGRATION ---------------------------
# Load necessary libraries
library(httr)
library(jsonlite)
library(dplyr)
library(readr)
# Define API URL
url <- "https://apis.vinmonopolet.no/stores/v0/details"
# Define your subscription key (replace with your actual key)
subscription_key <- "3b5b02c6793240fe9e6cb6d176e110e0"
# Send GET request with subscription key in header
response <- GET(url,
add_headers(
Accept = "application/json",
`Ocp-Apim-Subscription-Key` = subscription_key  # API authentication
))
# Check response status
if (status_code(response) == 200) {
# Convert API response to JSON and store it
data <- content(response, as = "text", encoding = "UTF-8")
store_data <- fromJSON(data)
# View first few rows
print(head(store_data))
} else {
print(paste("Error:", status_code(response)))
}
# --------------------------- Combine API with Vinmonopol Data ---------------------------
# Load necessary libraries
library(dplyr)
library(stringr)
library(tidyr) # Load tidyr for unnesting
library(tidyverse)
# Ensure store_data_clean is correctly formatted
store_data_clean <- store_data %>%
unnest_wider(address) %>%  # Expands nested address fields
select(
storeId,
storeName,
status,
postalCode,
city,
gpsCoord
) %>%
rename(
Store_ID = storeId,
Store_Name = storeName,
Store_Status = status,
Postal_Code = postalCode,
City = city,
GPS_Coordinates = gpsCoord
)
# Normalize the characters
# Transforming to normal characters
store_data_clean$Store_Name <- iconv(store_data_clean$Store_Name, from = "UTF-8", to = "ASCII//TRANSLIT")
store_data_clean$Store_Name <- trimws(store_data_clean$Store_Name)
# Standardize store names to improve matching
filtered_data <- filtered_data %>%
mutate(Store = str_trim(str_to_lower(Store)))  # Trim spaces and convert to lowercase
store_data_clean <- store_data_clean %>%
mutate(Store_Name = str_trim(str_to_lower(Store_Name)))  # Trim spaces and convert to lowercase
# Merge filtered_data (sales) with store_data_clean (store details)
final_data <- filtered_data %>%
left_join(store_data_clean, by = c("Store" = "Store_Name"))  # Match by store name
# Check merged data
head(final_data)
### Vinmonopolet data wrangling ###
# Importing libraries
library(tidyverse)
library(readxl)
library(writexl)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Define the path to your Excel file
file_path <- "Vinmonopolet_2024.xlsx"
# Get the names of all sheets in the Excel file
sheet_names <- excel_sheets(file_path)
# Read each sheet into a list of data frames, skipping the first row
list_of_dfs <- lapply(sheet_names, function(sheet) {
read_excel(file_path, sheet = sheet, skip = 2)
})
# Combine all data frames into a single data frame
combined_data <- bind_rows(list_of_dfs)
# View the combined data frame
print(combined_data)
# Unique values in the first column
unique_values <- unique(combined_data$...1)
print(unique_values)
# Transforming to normal characters
combined_data$...1 <- iconv(combined_data$...1, from = "UTF-8", to = "ASCII//TRANSLIT")
combined_data$...1 <- trimws(combined_data$...1)
# Define the values to filter out
values_to_exclude <- c(
"Svakvin", "Rodvin", "Hvitvin", "Musserende vin", "Rosevin",
"Perlende vin", "Aromatisert vin", "Sider", "Fruktvin",
"Brennevin", "Vodka", "Likor", "Whisky", "Akevitt",
"Brennevin, annet", "Gin", "Druebrennevin",
"Brennevin, noytralt < 37,5 %", "Rom", "Bitter",
"Fruktbrennevin", "Genever", "Ol", "Alkoholfritt", "Sterkvin", "Totalsum",
"eLager"
)
# Column names of combined data
colnames(combined_data)
# Filter out the specified values from the first column
filtered_data <- combined_data %>%
mutate("2024" = as.numeric(`2024`),
"Store" = as.character(`...1`)) %>%
filter(!.[[1]] %in% values_to_exclude) %>%
select("Store", "2024")
# View the filtered data
print(filtered_data)
# Export the filtered data to an Excel file
write_xlsx(filtered_data, "filtered_data.xlsx")
View(final_data)
