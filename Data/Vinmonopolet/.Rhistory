kurtosis_lsalary = kurtosis(`lsalary`, na.rm = TRUE)
)
# Histogram of log salary
ceosal2 %>%
ggplot(aes(x = lsalary)) +
geom_histogram(bins = 20, fill = "blue", alpha = 0.6) +
labs(title = "Distribution of CEO Salaries (Log Scale)", x = "Log Salary", y = "Count") +
theme_minimal()
# Boxplot of log salary
ceosal2 %>%
ggplot(aes(y = lsalary)) +
geom_boxplot(fill = "red", alpha = 0.6) +
labs(title = "Boxplot of CEO Salaries (Log Scale)", y = "Log Salary") +
theme_minimal()
# Correlation matrix excluding binary categorical variables
ceosal2 %>%
select(where(is.numeric), -college, -grad) %>%
cor() %>%
round(2) %>%
as_tibble(rownames = "Variable") %>%
kable(caption = "Correlation Matrix")
# Scatterplot for log sales vs log salary
ceosal2 %>%
ggplot(aes(x = lsales, y = lsalary)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", color = "red") +
labs(title = "CEO Salary vs Firm Sales (Log Scale)",
x = "Log Sales",
y = "Log Salary") +
theme_minimal()
# Scatterplot for log market value vs log salary
ceosal2 %>%
ggplot(aes(x = lmktval, y = lsalary)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", color = "blue") +
labs(title = "CEO Salary vs Market Value (Log Scale)",
x = "Log Market Value",
y = "Log Salary") +
theme_minimal()
# Scatterplot for Profits vs Log Salary
ceosal2 %>%
ggplot(aes(x = profits, y = lsalary)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", color = "green") +
labs(title = "CEO Salary vs Profits",
x = "Profits",
y = "Log Salary") +
theme_minimal()
# Create a cleaned dataset with selected predictors
ceosal2_clean <- ceosal1 %>%
select(lsalary, lsales, lmktval, profits, ceoten)
# Create a cleaned dataset with selected predictors
ceosal2_clean <- ceosal2 %>%
select(lsalary, lsales, lmktval, profits, ceoten)
# Print first few rows
head(ceosal2_clean)
# Ensure reproducibility
set.seed(123)
# Split data into training and test set
train_index <- createDataPartition(ceosal2_clean$lsalary, p = 0.7, list = FALSE)
train_data <- ceosal2_clean[train_index, ]
test_data <- ceosal2_clean[-train_index, ]
# Fit the model
lm_model <- lm(lsalary ~ ., data = train_data)
# Model summary
summary(lm_model)
# Predict on test data
lm_pred <- predict(lm_model, newdata = test_data)
# Calculate RMSE (Root Mean Squared Error)
lm_rmse <- sqrt(mean((test_data$lsalary - lm_pred)^2))
lm_rmse
# Predict on test data
lm_pred <- predict(lm_model, newdata = test_data)
# Calculate RMSE (Root Mean Squared Error)
lm_rmse <- sqrt(mean((test_data$lsalary - lm_pred)^2))
lm_rmse
# Prepare matrix for glmnet
X_train <- model.matrix(lsalary ~ ., data = train_data)[, -1] # Remove intercept column
y_train <- train_data$lsalary
X_test <- model.matrix(lsalary ~ ., data = test_data)[, -1]
# Perform LASSO using cross-validation to find optimal lambda
set.seed(123) # Ensure reproducibility
cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1) # alpha = 1 for LASSO
# Optimal lambda
optimal_lambda <- cv_lasso$lambda.min
# Fit LASSO model with optimal lambda
lasso_model <- glmnet(X_train, y_train, alpha = 1, lambda = optimal_lambda)
# Coefficients of LASSO model
lasso_coefficients <- coef(lasso_model)
lasso_coefficients
# Predict on test data
lasso_pred <- predict(lasso_model, s = optimal_lambda, newx = X_test)
# Calculate RMSE (Root Mean Squared Error)
lasso_rmse <- sqrt(mean((test_data$lsalary - lasso_pred)^2))
lasso_rmse
# Compare RMSE of Linear Regression and LASSO
comparison <- tibble(
Model = c("Linear Regression", "LASSO Regression"),
RMSE = c(lm_rmse, lasso_rmse))
comparison
# K-nearest neighbor
knn = function(x0, x, y, K = 10) {
ypred = sapply(x0, function(x_new) {
d = abs(x - x_new)  # Compute distances for each x_new
o = order(d)[1:K]   # Find indices of K nearest neighbors
mean(y[o])          # Compute the mean of their corresponding y values
})
return(ypred)
}
# Determining the optimal K using leave-one-out cross-validation
n = nrow(ceosal1)
K_values = 1:40
MSE_results = numeric(length(K_values))
# Loop over K values
for (j in 1:length(K_values)) {
K = K_values[j]
MSE1 = numeric(n)
# Leave-One-Out Cross-Validation
for (i in 1:n) {
train = ceosal1[-i, ]
test = ceosal1[i, ]
knn_pred = knn(test$lsales, train$lsales, train$lsalary, K)
MSE1[i] = (test$lsalary - knn_pred)^2
}
MSE_results[j] = mean(MSE1)
}
# Plot results
plot(K_values, MSE_results,
xlab = "K",
ylab = "Mean Squared Error (MSE) in LOOCV",
main = "K vs MSE in KNN Regression")
# K-nearest neighbor
knn = function(x0, x, y, K = 10) {
ypred = sapply(x0, function(x_new) {
d = abs(x - x_new)  # Compute distances for each x_new
o = order(d)[1:K]   # Find indices of K nearest neighbors
mean(y[o])          # Compute the mean of their corresponding y values
})
return(ypred)
}
# Determining the optimal K using leave-one-out cross-validation
n = nrow(ceosal2)
K_values = 1:40
MSE_results = numeric(length(K_values))
# Loop over K values
for (j in 1:length(K_values)) {
K = K_values[j]
MSE1 = numeric(n)
# Leave-One-Out Cross-Validation
for (i in 1:n) {
train = ceosal2[-i, ]
test = ceosal2[i, ]
knn_pred = knn(test$lsales, train$lsales, train$lsalary, K)
MSE1[i] = (test$lsalary - knn_pred)^2
}
MSE_results[j] = mean(MSE1)
}
# Plot results
plot(K_values, MSE_results,
xlab = "K",
ylab = "Mean Squared Error (MSE) in LOOCV",
main = "K vs MSE in KNN Regression")
# Predicting lsalary using optimal K
ceosal2$pred_lsalary <- knn(ceosal2$sales, ceosal2$sales, ceosal2$lsalary, K=26)
ceosal2
# Evaluating predictions using MSE:
mean((ceosal2$pred_lsalary-ceosal2$lsalary)^2)
# List of predictor variables to check
predictor_vars <- names(ceosal2_clean)[names(ceosal2_clean) != "lsalary"]
# Function to plot each predictor against lsalary with a smoothing line
plot_nonlinear_relationships <- function(data, response, predictors) {
for (var in predictors) {
p <- ggplot(data, aes_string(x = var, y = response)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "loess", color = "blue", se = TRUE) +
labs(title = paste("Relationship between", var, "and", response),
x = var, y = response) +
theme_minimal()
print(p)
}
}
# Plot the relationships
plot_nonlinear_relationships(ceosal2_clean, "lsalary", predictor_vars)
# Fit a linear model
lm_model <- lm(lsalary ~ ., data = ceosal2_clean)
# Summary of the linear model
summary(lm_model)
# Residual plots
par(mfrow = c(2, 2))
plot(lm_model)
# Partial residual plots
crPlots(lm_model)
# Fit a GAM model
gam_model <- gam(lsalary ~ s(age) + s(ceoten) + s(profits) + s(lsales) + s(lmktval), data = ceosal2_clean)
# Fit a GAM model
gam_model <- gam(lsalary ~ s(ceoten) + s(profits) + s(lsales) + s(lmktval), data = ceosal2_clean)
summary(gam_model)
# Refit the GAM model with significant smoothers
gam_model2 <- gam(lsalary ~ s(ceoten) + profits + s(lsales) + lmktval, data = ceosal2_clean)
summary(gam_model2)
# Compare AIC of linear and GAM models
AIC(lm_model, gam_model, gam_model2)
# Predicting lsalary using optimal K
ceosal2_clean$pred_lsalary <- knn(ceosal2_clean$lsales,
ceosal2_clean$lsales,
ceosal2_clean$lsalary,
K=26)
ceosal2
# Evaluating predictions using MSE:
mean((ceosal2$pred_lsalary-ceosal2$lsalary)^2)
# Predicting lsalary using optimal K
ceosal2_clean$pred_lsalary <- knn(ceosal2_clean$lsales,
ceosal2_clean$lsales,
ceosal2_clean$lsalary,
K=26)
ceosal2_clean
# Evaluating predictions using MSE:
mean((ceosal2_clean$pred_lsalary-ceosal2_clean$lsalary)^2)
# List of predictor variables to check
predictor_vars <- names(ceosal2_clean)[names(ceosal2_clean) != "lsalary"]
# Function to plot each predictor against lsalary with a smoothing line
plot_nonlinear_relationships <- function(data, response, predictors) {
for (var in predictors) {
p <- ggplot(data, aes_string(x = var, y = response)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "loess", color = "blue", se = TRUE) +
labs(title = paste("Relationship between", var, "and", response),
x = var, y = response) +
theme_minimal()
print(p)
}
}
# Plot the relationships
plot_nonlinear_relationships(ceosal2_clean, "lsalary", predictor_vars)
# List of predictor variables to check
predictor_vars <- names(ceosal2_clean)[names(ceosal2_clean) != c("lsalary", "pred_lsalary")]
# Function to plot each predictor against lsalary with a smoothing line
plot_nonlinear_relationships <- function(data, response, predictors) {
for (var in predictors) {
p <- ggplot(data, aes_string(x = var, y = response)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "loess", color = "blue", se = TRUE) +
labs(title = paste("Relationship between", var, "and", response),
x = var, y = response) +
theme_minimal()
print(p)
}
}
# Plot the relationships
plot_nonlinear_relationships(ceosal2_clean, "lsalary", predictor_vars)
View(train_data)
# Function to calculate RMSE
RMSE <- function(pred, actual) sqrt(mean((actual - pred)^2))
# Calculate RMSE of the linear regression model
lm_rmse <- RMSE(lm_pred, test_data$lsalary)
lm_rmse
# Calculate RMSE (Root Mean Squared Error)
lasso_rmse <- RMSE(lasso_pred, test_data$lsalary)
lasso_rmse
# Compare RMSE of Linear Regression and LASSO
comparison <- tibble(
Model = c("Linear Regression", "LASSO Regression"),
RMSE = c(lm_rmse, lasso_rmse))
comparison
# K-nearest neighbor
knn = function(x0, x, y, K = 10) {
ypred = sapply(x0, function(x_new) {
d = abs(x - x_new)  # Compute distances for each x_new
o = order(d)[1:K]   # Find indices of K nearest neighbors
mean(y[o])          # Compute the mean of their corresponding y values
})
return(ypred)
}
# Determining the optimal K using leave-one-out cross-validation
n = nrow(ceosal2)
K_values = 1:40
MSE_results = numeric(length(K_values))
# Loop over K values
for (j in 1:length(K_values)) {
K = K_values[j]
MSE1 = numeric(n)
# Leave-One-Out Cross-Validation
for (i in 1:n) {
train = ceosal2[-i, ]
test = ceosal2[i, ]
knn_pred = knn(test$lsales, train$lsales, train$lsalary, K)
MSE1[i] = (test$lsalary - knn_pred)^2
}
MSE_results[j] = mean(MSE1)
}
# Plot results
plot(K_values, MSE_results,
xlab = "K",
ylab = "Mean Squared Error (MSE) in LOOCV",
main = "K vs MSE in KNN Regression")
# Evaluating predictions using RMSE:
RMSE_knn <- RMSE(ceosal2$pred_lsalary, ceosal2$lsalary)
# Predicting lsalary using optimal K
ceosal2$pred_lsalary <- knn(ceosal2_clean$lsales,
ceosal2_clean$lsales,
ceosal2_clean$lsalary,
K=26)
ceosal2
# Evaluating predictions using RMSE:
RMSE_knn <- RMSE(ceosal2$pred_lsalary, ceosal2$lsalary)
RMSE_knn
# Residual plots
par(mfrow = c(2, 2))
plot(lm_model)
# Partial residual plots
crPlots(lm_model)
# Fit a GAM model
gam_model <- gam(lsalary ~ s(ceoten) + s(profits) + s(lsales) + s(lmktval), data = train_data)
summary(gam_model)
# Refit the GAM model with significant smoothers
gam_model2 <- gam(lsalary ~ s(ceoten) + profits + s(lsales) + lmktval, data = train_data)
summary(gam_model2)
# Compare AIC of linear and GAM models
AIC(lm_model, gam_model, gam_model2)
# Fit a GAM model
gam_model <- gam(lsalary ~ s(ceoten) + s(profits) + s(lsales) + s(lmktval), data = train_data)
summary(gam_model)
# Refit the GAM model with significant smoothers
gam_model2 <- gam(lsalary ~ s(ceoten) + profits + s(lsales) + lmktval, data = train_data)
summary(gam_model2)
# Compare AIC of linear and GAM models
AIC(lm_model, gam_model, gam_model2)
# RMSE for the first GAM model
rmse_gam1 <- RMSE(pred_gam1, test_data$lsalary)
# Predict the first GAM model
pred_gam1 <- predict(gam_model, newdata = test_data)
# RMSE for the first GAM model
rmse_gam1 <- RMSE(pred_gam1, test_data$lsalary)
# Predict the first GAM model
pred_gam1 <- predict(gam_model, newdata = test_data)
# RMSE for the first GAM model
rmse_gam1 <- RMSE(pred_gam1, test_data$lsalary)
# Predict the second GAM model
pred_gam2 <- predict(gam_model2, newdata = test_data)
# RMSE for the second GAM model
rmse_gam2 <- RMSE(pred_gam2, test_data$lsalary)
# Print RMSEs
cat("MSE for GAM Model 1:", rmse_gam1, "\n")
cat("MSE for GAM Model 2:", rmse_gam2, "\n")
cat("MSE for Linear Model:", lm_rmse, "\n")
# Compare RMSE of all models
comparison <- tibble(
Model = c("Linear Regression", "LASSO Regression", "KNN", "GAM Model 1", "GAM Model 2"),
RMSE = c(lm_rmse, lasso_rmse, RMSE_knn, rmse_gam1, rmse_gam2))
comparison
install.packages("gridExtra")
library(gridExtra)  # For arranging plots
# Function to plot each predictor against lsalary with a smoothing line
plot_nonlinear_relationships <- function(data, response, predictors) {
plots <- list()
for (var in predictors) {
p <- ggplot(data, aes_string(x = var, y = response)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "loess", color = "blue", se = TRUE) +
labs(title = paste("Relationship between", var, "and", response),
x = var, y = response) +
theme_minimal()
plots[[var]] <- p
}
do.call(grid.arrange, c(plots, ncol = 2))
}
# List of predictor variables to check
predictor_vars <- names(ceosal2_clean)[names(ceosal2_clean) != "lsalary"]
# Plot the relationships
plot_nonlinear_relationships(ceosal2_clean, "lsalary", predictor_vars)
rm(ceosal2_clean$pred_lsalary)
# Compare AIC of linear and GAM models
AIC(lm_model, gam_model)
# Predict the GAM model
pred_gam1 <- predict(gam_model, newdata = test_data)
# RMSE for the GAM model
rmse_gam1 <- RMSE(pred_gam1, test_data$lsalary)
rmse_gam1
# Compare RMSE of all models
comparison <- tibble(
Model = c("Linear Regression", "LASSO Regression", "KNN", "GAM Model"),
RMSE = c(lm_rmse, lasso_rmse, RMSE_knn, rmse_gam1))
comparison
setwd("C:/Users/Bruker/OneDrive - Norwegian School of Economics/NHH/8. semester/BAN440/Term paper/BAN440---Term-Paper/Data/Vinmonopolet")
### Bresnahan & Reiss test document ###
# relevant libraries
library(tidyverse)
library(readxl)
library(fastDummies)
library(knitr)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Load data
Vinmonopolet <- read_excel("final_data_mun.xlsx") %>%
select(-c(Store_ID, Store_Status, Postal_Code, GPS_Coordinates, Poststed,
PostnummerKategoriKode, PostnummerKategori, Region_Code,
Municipality_Name)) %>%
mutate(
Municipality_Name = Mun_name,
Region_Name = case_when(
Region_Name == "AUST-AGDER" ~ "Agder",
Region_Name == "VEST-AGDER" ~ "Agder",
Region_Name == "AKERSHUS" ~ "Akershus",
Region_Name == "OPPLAND" ~ "Innlandet",
Region_Name == "BUSKERUD" ~ "Buskerud",
Region_Name == "VESTFOLD" ~ "Vestfold",
Region_Name == "FINNMARK" ~ "Finnmark",
Region_Name == "HEDMARK" ~ "Innlandet",
Region_Name == "MØRE OG ROMSDAL" ~ "Møre og Romsdal",
Region_Name == "NORDLAND" ~ "Nordland",
Region_Name == "OSLO" ~ "Oslo",
Region_Name == "ROGALAND" ~ "Rogaland",
Region_Name == "TELEMARK" ~ "Telemark",
Region_Name == "TROMS" ~ "Troms",
Region_Name == "SØR-TRØNDELAG" ~ "Trøndelag",
Region_Name == "NORD-TRØNDELAG" ~ "Trøndelag",
Region_Name == "SOGN OG FJORDANE" ~ "Vestland",
Region_Name == "HORDALAND" ~ "Vestland",
Region_Name == "ØSTFOLD" ~ "Østfold",
is.na(Region_Name) & str_starts(Municipality_Code, "03") ~ "Oslo",
is.na(Region_Name) & str_starts(Municipality_Code, "11") ~ "Rogaland",
is.na(Region_Name) & str_starts(Municipality_Code, "15") ~ "Møre og Romsdal",
is.na(Region_Name) & str_starts(Municipality_Code, "18") ~ "Nordland",
is.na(Region_Name) & str_starts(Municipality_Code, "31") ~ "Østfold",
is.na(Region_Name) & str_starts(Municipality_Code, "32") ~ "Akershus",
is.na(Region_Name) & str_starts(Municipality_Code, "33") ~ "Buskerud",
is.na(Region_Name) & str_starts(Municipality_Code, "34") ~ "Innlandet",
is.na(Region_Name) & str_starts(Municipality_Code, "39") ~ "Vestfold",
is.na(Region_Name) & str_starts(Municipality_Code, "40") ~ "Telemark",
is.na(Region_Name) & str_starts(Municipality_Code, "42") ~ "Agder",
is.na(Region_Name) & str_starts(Municipality_Code, "46") ~ "Vestland",
is.na(Region_Name) & str_starts(Municipality_Code, "50") ~ "Trøndelag",
is.na(Region_Name) & str_starts(Municipality_Code, "55") ~ "Troms",
is.na(Region_Name) & str_starts(Municipality_Code, "56") ~ "Finnmark",
TRUE ~ Region_Name  # Keep existing Region_Name if no conditions are met
)
)
# Aggregating per market data for the Bresnahan & Reiss model
Vinmonopolet_market <- Vinmonopolet %>%
group_by(Municipality_Code) %>%
summarise(
Mun_name = first(Municipality_Name),
Region_Name = first(Region_Name),
Population = first(Population),
Area = first(Area),
Number_of_stores = sum(`2024` > 0),  # Count non-zero sales
Sales = sum(`2024`)
)
# Calculate rho, the raw correlation between Population and number of stores
rho <- cor(Vinmonopolet_market$Population, Vinmonopolet_market$Number_of_stores)
# Consumption per capita, grouped by region
Vinmonopolet_market %>%
group_by(Region_Name) %>%
summarise(
consumption = sum(Sales) / sum(Population),
Sales = sum(Sales)
) %>%
arrange(desc(consumption))
# Filtering data for B&R
br_data <- Vinmonopolet_market %>%
filter(Population < 150000 & Area > 0 & Population > 0)
# Table of the number of stores per market
table(br_data$Number_of_stores)
# Adding variables to the data
upperb <- 2
br_data <- br_data %>%
mutate(
s = Population / 1000,
density = Number_of_stores / Area,
Number_of_stores = as.factor(ifelse(Number_of_stores <= upperb, Number_of_stores, upperb))
) %>%
dummy_cols(select_columns = "Number_of_stores") %>%
mutate_at(vars(starts_with("Number_of_stores")), as.factor)
str(br_data)
# Fitting the Bresnahan & Reiss model
library(MASS)
model_1 <- polr(Number_of_stores ~ s, data = br_data, method = "probit")
summary(model_1)
model_2 <- polr(Number_of_stores ~ s + density, data = br_data, method = "probit")
## Model 1 ##
# Extract coefficients and cutoffs
lambda1 <- model_1$coefficients  # Estimate for s
theta1 <- model_1$zeta  # Cutoff points
# Compute S_N (Thresholds for s)
S_N1 <- exp(theta1)  # Since there is only one predictor, no need for mean adjustment
# Create labels for S_N
upperb1 <- length(theta1)  # Number of thresholds
slab1 <- paste0("$S_", 1:upperb1, "$")
names(S_N1) <- slab1
# Compute ETR_N using the cutoffs
ETR_N1 <- exp(theta1[2:upperb1] - theta1[1:(upperb1-1)]) * (1:(upperb1-1)) / (2:upperb1)
# Create labels for ETR_N
elab1 <- paste0("$s_", 2:upperb1, "/s_", 1:(upperb1-1), "$")
names(ETR_N1) <- elab1
# Print results
S_N1
ETR_N1
kable(S_N1, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
summary(model_1)
model_2 <- polr(Number_of_stores ~ s + density, data = br_data, method = "probit")
summary(model_2)
model_2 <- polr(Number_of_stores ~ s + density, data = br_data, method = "probit")
model_1 <- polr(Number_of_stores ~ log(s), data = br_data, method = "probit")
summary(model_1)
model_1 <- polr(Number_of_stores ~ s, data = br_data, method = "probit")
summary(model_1)
ETR_N1
View(Vinmonopolet)
# Regression model to test
reg <- lm(Number_of_stores ~ s + density, data = br_data)
