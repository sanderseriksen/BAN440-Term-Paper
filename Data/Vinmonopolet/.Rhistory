Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Load data
Vinmonopolet_market <- read_excel("B&R_data.xlsx")
# Calculate rho, the raw correlation between Population and number of stores
rho <- cor(Vinmonopolet_market$Population, Vinmonopolet_market$Number_of_stores)
## Data preparation ############################################################
# Filtering data for B&R
br_data <- Vinmonopolet_market %>%
filter(Population < 150000 & Area > 0 & Population > 0)
# Adding variables to the data
upperb <- 3
br_data <- br_data %>%
mutate(
s = Population / 1000,
log_s = log(s),
density = Number_of_stores / Area,
Number_of_stores = as.factor(ifelse(Number_of_stores <= upperb, Number_of_stores, upperb))
) %>%
dummy_cols(select_columns = "Number_of_stores") %>%
mutate_at(vars(starts_with("Number_of_stores")), as.factor)
# Scale the numeric variables
br_data <- br_data %>%
mutate_at(vars(Population, s, log_s, Area, Grensehandel, n_stays, Monthly_salary), scale)
# Correlation matrix
cor(br_data[, c("s", "log_s", "Area", "Grensehandel", "n_stays", "Monthly_salary", "Dist_nearest")])
## Some interesting statistics ################################################
# These justify adding the variables to the data
# Table of the number of stores per market
table(br_data$Number_of_stores)
# Consumption per capita, grouped by region
Vinmonopolet_market %>%
group_by(Region_Name) %>%
summarise(
consumption = sum(Sales) / sum(Population),
Sales = sum(Sales)
) %>%
arrange(desc(consumption))
# Regression model to test
reg <- lm(as.numeric(Number_of_stores) ~ Population + Area + Grensehandel + n_stays + Monthly_salary + Dist_nearest,
data = Vinmonopolet_market)
reg1 <- lm(as.numeric(Number_of_stores) ~ Population + Area + Grensehandel + n_stays + Monthly_salary + Dist_nearest,
data = br_data)
stargazer(reg, reg1, type = "text")
# Library necessary for the polr function.
# select() does not work after this is loaded
library(MASS)
# Model 1: Bresnahan & Reiss
model_1 <- polr(Number_of_stores ~ log_s, data = br_data, method = "probit")
summary(model_1)
# Extract coefficients and cutoffs
lambda1 <- model_1$coefficients  # Estimate for s
theta1 <- model_1$zeta  # Cutoff points
# Compute S_N (Thresholds for s)
S_N1 <- exp(theta1)  # Since there is only one predictor, no need for mean adjustment
# Create labels for S_N
upperb1 <- length(theta1)  # Number of thresholds
slab1 <- paste0("$S_", 1:upperb1, "$")
names(S_N1) <- slab1
# Compute ETR_N using the cutoffs
ETR_N1 <- exp(theta1[2:upperb1] - theta1[1:(upperb1-1)]) * (1:(upperb1-1)) / (2:upperb1)
# Create labels for ETR_N
elab1 <- paste0("$s_", 2:upperb1, "/s_", 1:(upperb1-1), "$")
names(ETR_N1) <- elab1
# Print results
S_N1
ETR_N1
kable(S_N1, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
# relevant libraries
library(tidyverse)
library(readxl)
library(fastDummies)
library(knitr)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Load data
Vinmonopolet_market <- read_excel("B&R_data.xlsx")
# Calculate rho, the raw correlation between Population and number of stores
rho <- cor(Vinmonopolet_market$Population, Vinmonopolet_market$Number_of_stores)
## Data preparation ############################################################
# Filtering data for B&R
br_data <- Vinmonopolet_market %>%
filter(Population < 150000 & Area > 0 & Population > 0)
# Adding variables to the data
upperb <- 3
br_data <- br_data %>%
mutate(
s = Population / 1000,
log_s = log(s),
density = Number_of_stores / Area,
Number_of_stores = as.factor(ifelse(Number_of_stores <= upperb, Number_of_stores, upperb))
) %>%
dummy_cols(select_columns = "Number_of_stores") %>%
mutate_at(vars(starts_with("Number_of_stores")), as.factor)
# Scale the numeric variables
#br_data <- br_data %>%
#  mutate_at(vars(Population, s, log_s, Area, Grensehandel, n_stays, Monthly_salary), scale)
# Correlation matrix
cor(br_data[, c("s", "log_s", "Area", "Grensehandel", "n_stays", "Monthly_salary", "Dist_nearest")])
## Some interesting statistics ################################################
# These justify adding the variables to the data
# Table of the number of stores per market
table(br_data$Number_of_stores)
# Consumption per capita, grouped by region
Vinmonopolet_market %>%
group_by(Region_Name) %>%
summarise(
consumption = sum(Sales) / sum(Population),
Sales = sum(Sales)
) %>%
arrange(desc(consumption))
# Library necessary for the polr function.
# select() does not work after this is loaded
library(MASS)
# Model 1: Bresnahan & Reiss
model_1 <- polr(Number_of_stores ~ log_s, data = br_data, method = "probit")
# Model 1: Bresnahan & Reiss
model_1 <- polr(Number_of_stores ~ log_s, data = br_data, method = "probit")
summary(model_1)
# Extract coefficients and cutoffs
lambda1 <- model_1$coefficients  # Estimate for s
theta1 <- model_1$zeta  # Cutoff points
# Compute S_N (Thresholds for s)
S_N1 <- exp(theta1)  # Since there is only one predictor, no need for mean adjustment
# Create labels for S_N
upperb1 <- length(theta1)  # Number of thresholds
slab1 <- paste0("$S_", 1:upperb1, "$")
names(S_N1) <- slab1
# Compute ETR_N using the cutoffs
ETR_N1 <- exp(theta1[2:upperb1] - theta1[1:(upperb1-1)]) * (1:(upperb1-1)) / (2:upperb1)
# Create labels for ETR_N
elab1 <- paste0("$s_", 2:upperb1, "/s_", 1:(upperb1-1), "$")
names(ETR_N1) <- elab1
# Print results
S_N1
ETR_N1
kable(S_N1, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ s + Dist_nearest, data = br_data, method = "probit")
# Display the summary of the model
summary(model_2)
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ s + Dist_nearest, data = br_data, method = "probit")
# Display the summary of the model
summary(model_2)
# Extract coefficients and cutoffs
lambda2 <- model_2$coefficients # Estimates for s and density
theta2 <- model_2$zeta # Cutoffs
# Compute S_N using the new predictors
S_N2 <- exp(theta2 - mean(br_data$Dist_nearest) * lambda2["Dist_nearest"])
# Create labels for S_N
upperb2 <- length(theta2) # Number of thresholds
slab2 <- paste0("$S_", 1:upperb2, "$")
names(S_N2) <- slab2
# Compute ETR_N using the cutoffs
ETR_N2 <- exp(theta2[2:upperb2] - theta2[1:(upperb2-1)]) * (1:(upperb2-1)) / (2:upperb2)
# Create labels for ETR_N
elab2 <- paste0("$s_", 2:upperb2, "/s_", 1:(upperb2-1), "$")
names(ETR_N2) <- elab2
# Print results
S_N2
ETR_N2
# Display the results in a table
kable(S_N2, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
# Display the frequency table of the Number_of_stores variable
table(br_data$Number_of_stores)
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ log_S + n_stays, data = br_data, method = "probit")
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ log_s + n_stays, data = br_data, method = "probit")
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ log_s + n_stays, data = br_data, method = "probit")
# Display the summary of the model
summary(model_2)
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ s + n_stays, data = br_data, method = "probit")
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ s + n_stays, data = br_data, method = "probit")
# Display the summary of the model
summary(model_2)
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ log_s + log(n_stays), data = br_data, method = "probit")
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ s + log(n_stays), data = br_data, method = "probit")
# Fit the model with the specified predictors
model_3 <- polr(Number_of_stores ~ log_s + Monthly_salary + Grensehandel + n_stays,
data = br_data, method = "probit")
## Model 3
# Fit the model with the specified predictors
model_3 <- polr(Number_of_stores ~ log_s + Monthly_salary + Grensehandel + n_stays,
data = br_data, method = "probit")
# Display the summary of the model
summary(model_3)
# Fit the model with the specified predictors
model_3 <- polr(Number_of_stores ~ s + Monthly_salary + Grensehandel + n_stays,
data = br_data, method = "probit")
# Display the summary of the model
summary(model_3)
# Scale the numeric variables
br_data <- br_data %>%
mutate_at(vars(Population, s, Area, Grensehandel, n_stays, Monthly_salary, Dist_nearest), scale)
# relevant libraries
library(tidyverse)
library(readxl)
library(fastDummies)
library(knitr)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Load data
Vinmonopolet_market <- read_excel("B&R_data.xlsx")
# Calculate rho, the raw correlation between Population and number of stores
rho <- cor(Vinmonopolet_market$Population, Vinmonopolet_market$Number_of_stores)
## Data preparation ############################################################
# Filtering data for B&R
br_data <- Vinmonopolet_market %>%
filter(Population < 150000 & Area > 0 & Population > 0)
# Adding variables to the data
upperb <- 3
br_data <- br_data %>%
mutate(
s = Population / 1000,
log_s = log(s),
density = Number_of_stores / Area,
Number_of_stores = as.factor(ifelse(Number_of_stores <= upperb, Number_of_stores, upperb))
) %>%
dummy_cols(select_columns = "Number_of_stores") %>%
mutate_at(vars(starts_with("Number_of_stores")), as.factor)
# Scale the numeric variables
br_data <- br_data %>%
mutate_at(vars(Population, s, Area, Grensehandel, n_stays, Monthly_salary, Dist_nearest), scale)
# Correlation matrix
cor(br_data[, c("s", "log_s", "Area", "Grensehandel", "n_stays", "Monthly_salary", "Dist_nearest")])
## Some interesting statistics ################################################
# These justify adding the variables to the data
# Table of the number of stores per market
table(br_data$Number_of_stores)
# Consumption per capita, grouped by region
Vinmonopolet_market %>%
group_by(Region_Name) %>%
summarise(
consumption = sum(Sales) / sum(Population),
Sales = sum(Sales)
) %>%
arrange(desc(consumption))
View(br_data)
# Library necessary for the polr function.
# select() does not work after this is loaded
library(MASS)
# Model 1: Bresnahan & Reiss
model_1 <- polr(Number_of_stores ~ s, data = br_data, method = "probit")
summary(model_1)
# Extract coefficients and cutoffs
lambda1 <- model_1$coefficients  # Estimate for s
theta1 <- model_1$zeta  # Cutoff points
# Compute S_N (Thresholds for s)
S_N1 <- exp(theta1)  # Since there is only one predictor, no need for mean adjustment
# Create labels for S_N
upperb1 <- length(theta1)  # Number of thresholds
slab1 <- paste0("$S_", 1:upperb1, "$")
names(S_N1) <- slab1
# Compute ETR_N using the cutoffs
ETR_N1 <- exp(theta1[2:upperb1] - theta1[1:(upperb1-1)]) * (1:(upperb1-1)) / (2:upperb1)
# Create labels for ETR_N
elab1 <- paste0("$s_", 2:upperb1, "/s_", 1:(upperb1-1), "$")
names(ETR_N1) <- elab1
# Print results
S_N1
ETR_N1
kable(S_N1, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ s + n_stays, data = br_data, method = "probit")
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ s + n_stays, data = br_data, method = "probit")
# Display the summary of the model
summary(model_2)
# Extract coefficients and cutoffs
lambda2 <- model_2$coefficients # Estimates for s and density
theta2 <- model_2$zeta # Cutoffs
# Compute S_N using the new predictors
S_N2 <- exp(theta2 - mean(br_data$n_stays) * lambda2["n_stays"])
# Create labels for S_N
upperb2 <- length(theta2) # Number of thresholds
slab2 <- paste0("$S_", 1:upperb2, "$")
names(S_N2) <- slab2
# Compute ETR_N using the cutoffs
ETR_N2 <- exp(theta2[2:upperb2] - theta2[1:(upperb2-1)]) * (1:(upperb2-1)) / (2:upperb2)
# Create labels for ETR_N
elab2 <- paste0("$s_", 2:upperb2, "/s_", 1:(upperb2-1), "$")
names(ETR_N2) <- elab2
# Print results
S_N2
ETR_N2
# Display the results in a table
kable(S_N2, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
# Display the frequency table of the Number_of_stores variable
table(br_data$Number_of_stores)
# Fit the model with the specified predictors
model_3 <- polr(Number_of_stores ~ s + Monthly_salary + Grensehandel + n_stays,
data = br_data, method = "probit")
## Model 3
# Fit the model with the specified predictors
model_3 <- polr(Number_of_stores ~ s + Monthly_salary + Grensehandel + n_stays,
data = br_data, method = "probit")
# Display the summary of the model
summary(model_3)
# Extract coefficients and cutoffs
lambda3 <- model_3$coefficients # Estimates for log_s, Monthly_salary, Grensehandel, and n_stays
theta3 <- model_3$zeta # Cutoffs
# Compute S_N using the sample means of all predictors
X_bar3 <- colMeans(br_data[, c("log_s", "Monthly_salary", "Grensehandel", "n_stays")])
S_N3 <- exp(theta3 - X_bar3 %*% lambda3)
# Create labels for S_N
upperb3 <- length(theta3) # Number of thresholds
slab3 <- paste0("$S_", 1:upperb3, "$")
names(S_N3) <- slab3
# Compute ETR_N using the cutoffs
ETR_N3 <- exp(theta3[2:upperb3] - theta3[1:(upperb3-1)]) * (1:(upperb3-1)) / (2:upperb3)
# Create labels for ETR_N
elab3 <- paste0("$s_", 2:upperb3, "/s_", 1:(upperb3-1), "$")
names(ETR_N3) <- elab3
# Display the results in a table
knitr::kable(S_N3, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds for Model 3',
booktabs = TRUE)
# Optionally, display the ETR_N3 in a table as well
knitr::kable(ETR_N3, col.names = c("ETR"), digits = 4,
caption = 'Entry Threshold Ratios for Model 3',
booktabs = TRUE)
## Model 3
# Fit the model with the specified predictors
model_3 <- polr(Number_of_stores ~ s + Monthly_salary + Grensehandel + n_stays,
data = br_data, method = "probit")
# Display the summary of the model
summary(model_3)
# Extract coefficients and cutoffs
lambda3 <- model_3$coefficients # Estimates for log_s, Monthly_salary, Grensehandel, and n_stays
theta3 <- model_3$zeta # Cutoffs
# Compute S_N using the sample means of all predictors
X_bar3 <- colMeans(br_data[, c("s", "Monthly_salary", "Grensehandel", "n_stays")])
S_N3 <- exp(theta3 - X_bar3 %*% lambda3)
# Create labels for S_N
upperb3 <- length(theta3) # Number of thresholds
slab3 <- paste0("$S_", 1:upperb3, "$")
names(S_N3) <- slab3
# Compute ETR_N using the cutoffs
ETR_N3 <- exp(theta3[2:upperb3] - theta3[1:(upperb3-1)]) * (1:(upperb3-1)) / (2:upperb3)
# Create labels for ETR_N
elab3 <- paste0("$s_", 2:upperb3, "/s_", 1:(upperb3-1), "$")
names(ETR_N3) <- elab3
# Display the results in a table
knitr::kable(S_N3, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds for Model 3',
booktabs = TRUE)
# Optionally, display the ETR_N3 in a table as well
knitr::kable(ETR_N3, col.names = c("ETR"), digits = 4,
caption = 'Entry Threshold Ratios for Model 3',
booktabs = TRUE)
# Model 1: Bresnahan & Reiss
model_1 <- polr(Number_of_stores ~ log_s, data = br_data, method = "probit")
summary(model_1)
# Model 1: Bresnahan & Reiss
model_1 <- polr(Number_of_stores ~ log_s, data = br_data, method = "probit")
summary(model_1)
# Extract coefficients and cutoffs
lambda1 <- model_1$coefficients  # Estimate for s
theta1 <- model_1$zeta  # Cutoff points
# Compute S_N (Thresholds for s)
S_N1 <- exp(theta1)  # Since there is only one predictor, no need for mean adjustment
# Create labels for S_N
upperb1 <- length(theta1)  # Number of thresholds
slab1 <- paste0("$S_", 1:upperb1, "$")
names(S_N1) <- slab1
# Compute ETR_N using the cutoffs
ETR_N1 <- exp(theta1[2:upperb1] - theta1[1:(upperb1-1)]) * (1:(upperb1-1)) / (2:upperb1)
# Create labels for ETR_N
elab1 <- paste0("$s_", 2:upperb1, "/s_", 1:(upperb1-1), "$")
names(ETR_N1) <- elab1
# Print results
S_N1
ETR_N1
kable(S_N1, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
## Model 2
# Fit the model with two predictors
model_2 <- polr(Number_of_stores ~ log_s + n_stays, data = br_data, method = "probit")
# Display the summary of the model
summary(model_2)
# Extract coefficients and cutoffs
lambda2 <- model_2$coefficients # Estimates for s and density
theta2 <- model_2$zeta # Cutoffs
# Compute S_N using the new predictors
S_N2 <- exp(theta2 - mean(br_data$n_stays) * lambda2["n_stays"])
# Create labels for S_N
upperb2 <- length(theta2) # Number of thresholds
slab2 <- paste0("$S_", 1:upperb2, "$")
names(S_N2) <- slab2
# Compute ETR_N using the cutoffs
ETR_N2 <- exp(theta2[2:upperb2] - theta2[1:(upperb2-1)]) * (1:(upperb2-1)) / (2:upperb2)
# Create labels for ETR_N
elab2 <- paste0("$s_", 2:upperb2, "/s_", 1:(upperb2-1), "$")
names(ETR_N2) <- elab2
# Print results
S_N2
ETR_N2
# Display the results in a table
kable(S_N2, col.names = c("'000s"), digits = 4,
caption = 'Entry thresholds',
booktabs = TRUE)
# Display the frequency table of the Number_of_stores variable
table(br_data$Number_of_stores)
# relevant libraries
library(tidyverse)
library(readxl)
library(fastDummies)
library(knitr)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Load data
Vinmonopolet_market <- read_excel("B&R_data.xlsx")
# Calculate rho, the raw correlation between Population and number of stores
rho <- cor(Vinmonopolet_market$Population, Vinmonopolet_market$Number_of_stores)
## Data preparation ############################################################
# Filtering data for B&R
#br_data <- Vinmonopolet_market %>%
#  filter(Population < 150000 & Area > 0 & Population > 0)
# Adding variables to the data
upperb <- 3
br_data <- br_data %>%
mutate(
s = Population / 1000,
log_s = log(s),
density = Number_of_stores / Area,
Number_of_stores = as.factor(ifelse(Number_of_stores <= upperb, Number_of_stores, upperb))
) %>%
dummy_cols(select_columns = "Number_of_stores") %>%
mutate_at(vars(starts_with("Number_of_stores")), as.factor)
library(stargazer)
#### Bresnahan & Reiss test document ####
# relevant libraries
library(tidyverse)
library(readxl)
library(fastDummies)
library(knitr)
library(stargazer)
# Set locale to UTF-8
Sys.setlocale("LC_ALL", "en_US.UTF-8")
# Load data
Vinmonopolet_market <- read_excel("B&R_data.xlsx")
# Calculate rho, the raw correlation between Population and number of stores
rho <- cor(Vinmonopolet_market$Population, Vinmonopolet_market$Number_of_stores)
## Data preparation ############################################################
# Filtering data for B&R
br_data <- Vinmonopolet_market %>%
filter(Population < 150000 & Area > 0 & Population > 0)
# Adding variables to the data
upperb <- 3
br_data <- br_data %>%
mutate(
s = Population / 1000,
log_s = log(s),
density = Number_of_stores / Area,
Number_of_stores = as.factor(ifelse(Number_of_stores <= upperb, Number_of_stores, upperb))
) %>%
dummy_cols(select_columns = "Number_of_stores") %>%
mutate_at(vars(starts_with("Number_of_stores")), as.factor)
# Scale the numeric variables
#br_data <- br_data %>%
#  mutate_at(vars(Population, s, Area, Grensehandel, n_stays, Monthly_salary, Dist_nearest), scale)
# Correlation matrix
cor(br_data[, c("s", "log_s", "Area", "Grensehandel", "n_stays", "Monthly_salary", "Dist_nearest")])
## Some interesting statistics ################################################
# These justify adding the variables to the data
# Table of the number of stores per market
table(br_data$Number_of_stores)
# Consumption per capita, grouped by region
Vinmonopolet_market %>%
group_by(Region_Name) %>%
summarise(
consumption = sum(Sales) / sum(Population),
Sales = sum(Sales)
) %>%
arrange(desc(consumption))
# Regression model to test
reg <- lm(as.numeric(Number_of_stores) ~ Population + Area + Grensehandel + n_stays + Monthly_salary + Dist_nearest,
data = Vinmonopolet_market)
reg1 <- lm(as.numeric(Number_of_stores) ~ Population + Area + Grensehandel + n_stays + Monthly_salary + Dist_nearest,
data = br_data)
stargazer(reg, reg1, type = "text")
# Regression model to test
reg <- lm(as.numeric(Number_of_stores) ~ s + Area + Grensehandel + n_stays + Monthly_salary + Dist_nearest,
data = Vinmonopolet_market)
